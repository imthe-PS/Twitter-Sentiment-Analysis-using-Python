{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "\n",
    "import  tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#Accessing the twitter API keys\n",
    "\n",
    "CONSUMER_KEY = \"4l1xxxxxxxxxxxxxxxKTFW3\"\n",
    "CONSUMER_SECRET = \"uvdSMxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxvG3L1\"\n",
    "ACCESS_TOKEN = \"xxxxxxxxxxxx873143296-z7qkGxxxxxxxxfvxFIFAGTm8hDwh\"\n",
    "ACCESS_SECRET = \"kCYxxxxxxxxxxx6zNIAfnBkHphrEdKpKTQVyxxxxxxG\"\n",
    "\n",
    "#importing access keys\n",
    "\n",
    "def twitter_setup():\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "    api = tweepy.API(auth)\n",
    "    return api\n",
    "\n",
    "# Extract 100 tweets from the twitter user suing twitter API's(Use username, name, hashtag as required)\n",
    "\n",
    "extractor = twitter_setup()\n",
    "posts = extractor.user_timeline(screen_name=\"@realDonaldTrump\", count = 100, lang =\"en\",tweet_mode=\"extended\")\n",
    "\n",
    "#  Print 5 recent tweets\n",
    "\n",
    "print(\"no. of tweets extracted: {}.\\n\".format(len(posts)))\n",
    "print(\"Show the 5 recent tweets:\\n\")\n",
    "i=1\n",
    "for tweet in posts[:5]:\n",
    "    print(str(i) +') '+ tweet.full_text + '\\n')\n",
    "    i= i+1\n",
    "    \n",
    "#Creating pandas DataFrame\n",
    "\n",
    "data = pd.DataFrame(data=[tweet.full_text for tweet in posts], columns=['tweets'])\n",
    "display(data.head(10))\n",
    "\n",
    "# Adding relevant data to the dataframe\n",
    "\n",
    "data['len'] = np.array([len(tweet.full_text) for tweet in posts])\n",
    "data['ID'] = np.array([tweet.id for tweet in posts])\n",
    "data['Date'] = np.array([tweet.created_at for tweet in posts])\n",
    "data['Source'] = np.array([tweet.source for tweet in posts])\n",
    "data['Likes'] = np.array([tweet.favorite_count for tweet in posts])\n",
    "data['Re_tweets'] = np.array([tweet.retweet_count for tweet in posts])\n",
    "data.head() \n",
    "\n",
    "# We extract the tweet with more LIKEs and more RE_TWEETS\n",
    "\n",
    "likes_max = np.max(data['Likes'])\n",
    "re_tweets_max = np.max(data['Re_tweets'])\n",
    "likes = data[data.Likes == likes_max].index[0]\n",
    "re_tweets = data[data.Re_tweets == re_tweets_max].index[0]\n",
    "\n",
    "# Max LIKES\n",
    "print(\"The tweet with more likes is: \\n{}\".format(data['tweets'][likes]))\n",
    "print(\"Number of likes: {}\".format(likes_max))\n",
    "print(\"{} character \\n\".format(data['len'][likes]))\n",
    "\n",
    "# Max RE_TWEETS\n",
    "print(\"The tweet with more re_tweets is: \\n{}\".format(data['tweets'][re_tweets]))\n",
    "print(\"Number of retweets: {}\".format(re_tweets_max))\n",
    "print(\"{} character.\\n\".format(data['len'][re_tweets]))\n",
    "\n",
    "#Plotting Graph based on tweets and it's length\n",
    "\n",
    "tweet_len = pd.Series(data = data['len'].values)\n",
    "tweet_likes = pd.Series(data = data['Likes'].values)   #, index=data['Data'])\n",
    "tweet_retweets = pd.Series(data = data['Re_tweets'].values) #, index=data['Data']\n",
    "x = data.index\n",
    "y = data['len']\n",
    "plt.xlabel(\"tweet number\")\n",
    "plt.ylabel(\"length of tweet\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "plt.plot(x, y, 'go--')\n",
    "plt.show()\n",
    "\n",
    "# Likes and retweets visualization\n",
    "\n",
    "tweet_likes.plot(figsize=(20,4), label='Likes', legend=True)\n",
    "tweet_retweets.plot(figsize=(20,4), label='Re_tweets', legend=True)\n",
    "\n",
    "# We obtain creation of conent sources:\n",
    "\n",
    "sources = []\n",
    "for source in data['Source']:\n",
    "    if source not in sources:\n",
    "        sources.append(source)\n",
    "# We print sources list:\n",
    "print(\"Creation of content sources:\")\n",
    "for source in sources:\n",
    "    print(\"* {}\".format(source))\n",
    "    \n",
    "# Plotting the usage of sources:\n",
    "percent = np.zeros(len(sources))\n",
    " \n",
    "for source in data['Source']:\n",
    "    for index in range(len(sources)):\n",
    "        if source == sources[index]:\n",
    "            percent[index] += 1\n",
    "            pass\n",
    " \n",
    "percent /= 100\n",
    "# Pie chart:\n",
    "pie_chart = pd.Series(percent, index=sources, name='Sources')\n",
    "pie_chart.plot.pie(fontsize=11, autopct='%.2f', figsize=(6, 6))\n",
    "\n",
    "#Removing data otherthan Alphabets\n",
    "\n",
    "def remove_pattern(data, pattern):\n",
    "    r = re.findall(pattern, data)\n",
    "    for i in r:\n",
    "        data = re.sub(i, '', data)   \n",
    "    return data   \n",
    "# remove twitter handles (@user)\n",
    "data['cleaned_tweets'] = np.vectorize(remove_pattern)(data['tweets'], \"@[\\w]*\")\n",
    "#Removing Punctuations, Numbers, and Special Characters\n",
    "data['cleaned_tweets'] = data['cleaned_tweets'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "data['cleaned_tweets'] = data['cleaned_tweets'].str.replace(\"[https]\", \" \")\n",
    "#Removing non-english words\n",
    "def preprocess2(cleaned_tweets):\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    return\n",
    "#Removing Short Words\n",
    "data['cleaned_tweets'] = data['cleaned_tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "data\n",
    "\n",
    "# Create a function to get the subjectivity and polarity\n",
    "\n",
    "def getSubjectivity(text):\n",
    "   return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "# Create a function to get the polarity\n",
    "def getPolarity(text):\n",
    "   return  TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Create two new columns 'Subjectivity' & 'Polarity'\n",
    "data['Subjectivity'] = data['tweets'].apply(getSubjectivity)\n",
    "data['Polarity'] = data['tweets'].apply(getPolarity)\n",
    "\n",
    "# Show the new dataframe with columns 'Subjectivity' & 'Polarity'\n",
    "data\n",
    "\n",
    "#Tokenization\n",
    "#Now we will tokenize all the cleaned tweets in our dataset. \n",
    "\n",
    "tokenized_tweet = data['cleaned_tweets'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()\n",
    "\n",
    "#Now letâ€™s stitch these tokens back together.\n",
    "\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "data['cleaned_tweets'] = tokenized_tweet\n",
    "\n",
    "# Displaying all words of the tweets\n",
    "\n",
    "all_words = ' '.join([text for text in data['cleaned_tweets']])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(background_color = 'white', width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# non-racist/sexist words\n",
    "normal_words =' '.join([text for text in data['cleaned_tweets'][data['Polarity'] == 0]])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Racist words in words\n",
    "negative_words = ' '.join([text for text in data['cleaned_tweets'][data['Polarity'] == 1]])\n",
    "wordcloud = WordCloud(background_color = 'white', width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Create a function to compute negative (-1), neutral (0) and positive (+1) analysis based on polarity\n",
    "def getAnalysis(score):\n",
    "    if score < 0:\n",
    "        return 'Negative'\n",
    "    elif score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "         return 'Positive'\n",
    "data['Analysis'] = data['Polarity'].apply(getAnalysis)\n",
    "# Show the dataframe\n",
    "data\n",
    "\n",
    "\n",
    "# We construct lists with classified tweets to get the percentages\n",
    "\n",
    "pos_tweets = [ tweet for index, tweet in enumerate(data['tweets']) if data['Polarity'][index] > 0]\n",
    "neu_tweets = [ tweet for index, tweet in enumerate(data['tweets']) if data['Polarity'][index] == 0]\n",
    "neg_tweets = [ tweet for index, tweet in enumerate(data['tweets']) if data['Polarity'][index] < 0]\n",
    "\n",
    "# We print percentages:\n",
    "\n",
    "print(\"Percentage of positive tweets: {}%\".format(len(pos_tweets)*100/len(data['tweets'])))\n",
    "print(\"Percentage of neutral tweets: {}%\".format(len(neu_tweets)*100/len(data['tweets'])))\n",
    "print(\"Percentage de negative tweets: {}%\".format(len(neg_tweets)*100/len(data['tweets'])))\n",
    "\n",
    "# Finding the number of positive,negative and neutral tweets and plot them in bar chart\n",
    "data['Analysis'].value_counts()\n",
    "\n",
    "plt.title('Number count', fontsize=20)\n",
    "plt.xlabel('Type of emotion', fontsize=16)\n",
    "plt.ylabel('count', fontsize=16)\n",
    "data['Analysis'].value_counts().plot(kind = 'bar', color='cyan', edgecolor='black')\n",
    "plt.show()\n",
    "\n",
    "# To get the emotion of a particular tweet\n",
    "data.iloc[120,[7, 10]] #120 = row, [7,10] = columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
